# Текстовый трансформер на 4 символа

Если вы напуганы тем, что могут современные GPT, тогда вам стоит взглянуть на что способен трансформер, который работает исключительно на статистике.

Этот трансформер обладает контекстом всего в **3 символа**.

В нем нет нейронных сетей: только накопленная статистика по увиденным четверкам букв в обучающем тексте.

Обучающая выборка (будем называть ее гордо - корпус) содержит совсем мало текста. Здесь есть несколько на выбор. Мультиязычность при таком контексте лучше не надо.

Вот пример вывода при обучении на собрании стихов Пушкина:

```
>> Когда

Когда страх!.. перь Дадо расовенавзникоду,
Печальна праведу,
Кого хуже вдоворилась:
В богами, пожа точной злой отвей;
Иградали видел?
С сиян, велая,
В дорок плечи.
Ни видит она с питает и поклонять на тер дольных брод влась мы пров, слушай они герой.
Узнать,
К постоянный денно,
С сировинное!..»
И замостих посылки». Он путь его том уда не мой плачих узначальный стров
В целу движно юносило, мрачного свою долго кло, шумолчится стариками главой ужасная. Сердце холмах меня пыльцо,
А кажигала, небовью сести буйной,
Увы! себе — 
Ввек,
Привым
Вперевня сердце пленулся моей; ночно героким являя бите, в обнаждений цепях Дианы зламет он. Снеспод буду, чья конишь богитерялась,
Сия путь желес
.... Боваясь, пословень!
Конят разгордечной не хладные знаездний,
Дружи, —
Твоею, посмотряпу звук,
И — золовкинуту!
1-й    г он в доно двершился. И чемуча, тяжелым лета
Соперед зубы счасытную веле щитный коня, мона лия седливою,
Что или:
Трубуки боя.
«Попасиле следною прижды просклон не он лей на девы в толпой
Не призывал, 
Достелы разалос
```

Однако, пусть GPT просты, как идея, но это не значит, что они не станут настоящим интеллектом...
